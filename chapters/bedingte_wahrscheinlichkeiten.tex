\chapter{Bedingte Wahrscheinlichkeiten}

Durch das Bekanntwerden von zusätzlichen Informationen verändern sich die Wahrscheinlichkeiten. Manche
Elementarereignisse werden wahrscheinlicher, andere unwahrscheinlicher oder sogar unmöglich.

\begin{definition}
    $A$ und $B$ seien Ereignisse mit Pr$[B] > 0$. Die \textbf{bedingte Wahrscheinlichkeit} Pr$[A | B]$
    ($A$ gegeben $B$) ist definiert als:

    $$\text{Pr}[A | B] := \frac{\text{Pr}[A \cap B]}{\text{Pr}[B]}$$

\end{definition}
\bigskip

Häufig wird die Definition der bedingten Wahrscheinlichkeit auch wie folgt benutzt:
$$\text{Pr}[A \cap B] = \text{Pr}[A | B] \cdot \text{Pr}[B] = \text{Pr}[B | A] \cdot \text{Pr}[A]$$
Für mehrere Ereignisse führt dies zu folgender Rechenregel:

\begin{satz}[Multiplikationssatz]
    Gegeben Ereignisse $A_1, A_2, ... , A_n$. Falls Pr$[A_1 \cap A_2 \cap ... \cap A_n] > 0$ ist, so 
    gilt:
    $$\text{Pr}[A_1 \cap ... \cap A_n] = \text{Pr}[A_1] \cdot \text{Pr}[A_2 | A_1] \cdot \text{Pr}[A_3 | A_1 \cap A_2] ...\text{Pr}[A_n | A_1 \cap ... \cap A_{n-1}]$$
\end{satz}
\bigskip

Dieser Satz kommt vorallem bei \textit{Balls into Bins} Problemen zum gebrauch: Man werfe $m$ Bälle
in $n$ Körbe, wie gross ist die Wahrscheinlichkeit, dass am Ende alle Bälle allein in einem Korb liegen? \\

Basierend auf der Definition der bedingten Wahrscheinlichkeit gibt es auch den folgenden Satz zu zeigen:

\begin{satz}[Satz der totalen Wahrscheinlichkeite]
    Die Ereignisse $A_1, A_2, ... , A_n$ seien paarweise disjunkt und es gelte $B \subseteq A_1 \cup A_2 \cup ... \cup A_n$.
    Dann gilt folgendes:

    $$\text{Pr}[B] = \sum_{i = 1}^{n}\text{Pr}[B | A_i]\cdot\text{Pr}[A_i]$$
\end{satz}
\bigskip

Dieser Satz lässt sich relativ leicht begründen. Jeder einzelne Summand ist gleich $B \cup A_i$. 
Durch die Bedingungen von paarweiser Disjunktion und dass $B$ Untermenge der Vereinigung von $A$ sein 
muss, ergibt das Aufsummieren aller Summanden genau die Wahrscheinlichkeit der Menge $B$. Dieser Satz 
kann für das Monty-Hall Problem verwendet werden. \\

Als letzter Satz in diesem Kapitel schauen wir nun noch den Satz von Bayes an:

\begin{satz}[Satz von Bayes]
    Die Ereignisse $A_1, ... , A_n$ seien paarweise dijunkt und es gelte $B \subseteq A_1 \cup ... \cup A_n$
    sowie Pr$[B] > 0$. Dann gilt (für $1 \geq i \geq n$):

    $$\text{Pr}[A_i | B] = \frac{\text{Pr}[A_i \cap B]}{\text{Pr}[B]} = \frac{\text{Pr}[B | A_i] \cdot \text{Pr}[A_i]}{\sum_{j = 1}^{n}\text{Pr}[B | A_j] \cdot \text{Pr}[A_j]}$$
\end{satz}
\bigskip

Dieser Satz wird vorallem oft mit dem Beispiel von medizinischen Tests betrachtet. Definiert man die
Ereignisse $P$ = Test positiv, $N$ = Test negativ, $G$ = gesund und $K = krank$, so ist es oft einfach
die Wahrscheinlichkeiten für Pr$[P | K]$ und Pr$[P | G]$ zu ermitteln. Was jedoch oftmals für die 
Patienten spannender ist, ist die Wahrscheinlichkeit Pr$[K | P]$, hierfür können wir nun den Satz von
Bayes verwenden. 